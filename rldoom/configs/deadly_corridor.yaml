# rldoom/configs/deadly_corridor.yaml

env:
  cfg_path: "doom_files/deadly_corridor.cfg"
  wad_path: "doom_files/deadly_corridor.wad"
  frame_size: 84
  stack_size: 4
  frame_skip: 4

train:
  num_episodes: 2000              
  max_steps_per_episode: 3000     # ← both off/on 
  checkpoint_dir: "checkpoints"
  checkpoint_interval: 100        # ← 2000/100 = 20 ckpts
  logs_dir: "logs"

defaults:
  feature_dim: 512
  gamma: 0.99
  grad_clip: 10.0

logging:
  use_wandb: true
  wandb_project: "RLDoom"
  wandb_entity: "lee_changmin-sangmyung-uni"

algos:
  # ---------- Off-policy ----------
  dqn:
    type: "offpolicy"
    lr: 0.0001
    buffer_size: 100000
    batch_size: 128
    learn_start: 5000
    eps_start: 1.0
    eps_end: 0.01
    eps_decay: 100000
    target_update_every: 1000

  ddqn:
    type: "offpolicy"
    lr: 0.0001
    buffer_size: 100000
    batch_size: 128
    learn_start: 5000
    eps_start: 1.0
    eps_end: 0.01
    eps_decay: 100000
    target_update_every: 1000

  dddqn:
    type: "offpolicy"
    lr: 0.0001
    buffer_size: 100000
    batch_size: 128
    learn_start: 5000
    eps_start: 1.0
    eps_end: 0.01
    eps_decay: 100000
    target_update_every: 1000

  rainbow:
    type: "offpolicy"
    lr: 0.0001
    buffer_size: 100000
    batch_size: 128
    learn_start: 5000
    eps_start: 1.0
    eps_end: 0.01
    eps_decay: 100000
    target_update_every: 1000
    per_alpha: 0.6
    per_beta_start: 0.4
    per_beta_frames: 1000000

  # ---------- On-policy ----------
  reinforce:
    type: "onpolicy"
    lr: 0.0001
    gamma: 0.99
    ent_coef: 0.0
    batch_size: 128

  a2c:
    type: "onpolicy"
    lr_actor: 0.0001
    lr_critic: 0.0001
    ent_coef: 0.01
    vf_coef: 0.5
    gamma: 0.99
    rollout_len: 128
    batch_size: 128

  a3c:
    type: "onpolicy"
    lr_actor: 0.0001
    lr_critic: 0.0001
    ent_coef: 0.01
    vf_coef: 0.5
    gamma: 0.99
    rollout_len: 128
    batch_size: 128

  ppo:
    type: "onpolicy"
    lr_actor: 0.0001
    lr_critic: 0.0001
    ent_coef: 0.01
    vf_coef: 0.5
    gamma: 0.99
    gae_lambda: 0.95
    rollout_len: 128
    batch_size: 128
    ppo_epochs: 4
    clip_range: 0.2

  trpo:
    type: "onpolicy"
    lr_actor: 0.0001
    lr_critic: 0.0001
    ent_coef: 0.01
    vf_coef: 0.5
    gamma: 0.99
    gae_lambda: 0.95
    rollout_len: 128
    batch_size: 128
    trpo_epochs: 4
    kl_coef: 0.5
